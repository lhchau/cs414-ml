# Transformer 

## Operation

### Multi-head Self-Attention

### Layer Norm

### Residual Connections

### Attention logit scaling

## Transformer Encoder

## Transformer Decoder